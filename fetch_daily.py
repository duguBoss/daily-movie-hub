import requests
import json
import os
import sys
import shutil
import wikipedia
import urllib.parse
from bs4 import BeautifulSoup
from datetime import datetime

# ---------------- é…ç½®åŒºåŸŸ ----------------
TOKEN = os.environ.get("TMDB_API_KEY")
if not TOKEN:
    print("âŒ é”™è¯¯: ç¯å¢ƒå˜é‡ TMDB_API_KEY æœªæ‰¾åˆ°ï¼")
    sys.exit(1)

HEADERS = {
    "accept": "application/json",
    "Authorization": f"Bearer {TOKEN}"
}

BASE_URL = "https://api.themoviedb.org/3"
IMAGE_BASE_URL = "https://image.tmdb.org/t/p/w500"
BACKDROP_BASE_URL = "https://image.tmdb.org/t/p/original"

DATA_DIR = "data"
IMAGES_DIR = os.path.join(DATA_DIR, "images")
JSON_FILE = os.path.join(DATA_DIR, "weekly_updates.json")

# è®¾ç½®ç»´åŸºç™¾ç§‘è¯­è¨€ä¸ºä¸­æ–‡
wikipedia.set_lang("zh")

# ---------------- è¾…åŠ©åŠŸèƒ½ ----------------

def setup_directories(reset=False):
    if reset:
        print("ğŸ”„ å‘¨ä¸€é‡ç½®: æ¸…ç†æ—§æ•°æ®...")
        if os.path.exists(DATA_DIR):
            shutil.rmtree(DATA_DIR)
    
    os.makedirs(IMAGES_DIR, exist_ok=True)
    if not os.path.exists(JSON_FILE):
        with open(JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump([], f)

def load_existing_ids():
    if os.path.exists(JSON_FILE):
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                return [item["id"] for item in data]
            except:
                return []
    return []

def download_image(url, filename):
    if not url: return None
    try:
        resp = requests.get(url, timeout=15)
        if resp.status_code == 200:
            file_path = os.path.join(IMAGES_DIR, filename)
            with open(file_path, "wb") as f:
                f.write(resp.content)
            return f"images/{filename}"
    except:
        pass
    return None

# ---------------- ä¿¡æ¯å¢å¼ºæ¨¡å— (Wiki & Baidu) ----------------

def get_wikipedia_summary(query):
    """
    è·å–ç»´åŸºç™¾ç§‘ç®€ä»‹ (æ— éœ€API Key)
    """
    print(f"   ğŸ” å°è¯•æœç´¢ Wiki: {query}...")
    try:
        # æœç´¢æœ€åŒ¹é…çš„æ¡ç›®
        search_results = wikipedia.search(query)
        if not search_results:
            return ""
        
        # è·å–ç¬¬ä¸€ä¸ªç»“æœçš„é¡µé¢
        page = wikipedia.page(search_results[0], auto_suggest=False)
        summary = page.summary[:600] # æˆªå–å‰600å­—
        return f"{summary}...\n(ğŸ“š æ¥æº: ç»´åŸºç™¾ç§‘)"
    
    except wikipedia.exceptions.DisambiguationError as e:
        # å¦‚æœé‡åˆ°æ­§ä¹‰ï¼ˆä¾‹å¦‚â€œç‹‚é£™â€æœ‰ç”µè§†å‰§å’Œè¯è¯­ï¼‰ï¼Œå°è¯•å–ç¬¬ä¸€ä¸ªé€‰é¡¹
        try:
            page = wikipedia.page(e.options[0], auto_suggest=False)
            return f"{page.summary[:600]}...\n(ğŸ“š æ¥æº: ç»´åŸºç™¾ç§‘)"
        except:
            return ""
    except Exception as e:
        # print(f"   Wikiè·å–å¾®å°é”™è¯¯: {e}") # å¿½ç•¥éè‡´å‘½é”™è¯¯
        return ""

def get_baidu_baike_summary(title):
    """
    è·å–ç™¾åº¦ç™¾ç§‘ç®€ä»‹ (é€šè¿‡çˆ¬è™«ï¼Œæ— éœ€API Key)
    """
    print(f"   ğŸ” å°è¯•æœç´¢ç™¾åº¦ç™¾ç§‘: {title}...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        # URL ç¼–ç 
        encoded_title = urllib.parse.quote(title)
        url = f"https://baike.baidu.com/item/{encoded_title}"
        
        resp = requests.get(url, headers=headers, timeout=5)
        resp.encoding = 'utf-8' # å¼ºåˆ¶utf-8
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            # ç™¾åº¦ç™¾ç§‘ç®€ä»‹çš„å¸¸è§ class
            summary_div = soup.find("div", class_="lemma-summary") or \
                          soup.find("div", class_="lemma-summary-box") or \
                          soup.find("div", attrs={"class": lambda x: x and "lemmaSummary" in x})
            
            if summary_div:
                text = summary_div.get_text().strip().replace("\n", "").replace("\xa0", "")
                return f"{text[:600]}...\n(ğŸ¼ æ¥æº: ç™¾åº¦ç™¾ç§‘)"
    except Exception:
        pass
    return ""

def get_english_fallback(media_type, media_id):
    """è·å– TMDB è‹±æ–‡ç®€ä»‹ä½œä¸ºä¿åº•"""
    url = f"{BASE_URL}/{media_type}/{media_id}"
    try:
        resp = requests.get(url, headers=HEADERS, params={"language": "en-US"}, timeout=5)
        return resp.json().get("overview", "")
    except:
        return ""

def get_external_ids(media_type, media_id):
    """è·å–å¤–éƒ¨ID (IMDb)"""
    url = f"{BASE_URL}/{media_type}/{media_id}/external_ids"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=5)
        return resp.json()
    except:
        return {}

# ---------------- æ ¸å¿ƒè·å–é€»è¾‘ ----------------

def get_credits(media_type, media_id):
    url = f"{BASE_URL}/{media_type}/{media_id}/credits"
    try:
        resp = requests.get(url, headers=HEADERS, params={"language": "zh-CN"}, timeout=5)
        data = resp.json()
        directors = [c["name"] for c in data.get("crew", []) if c["job"] == "Director"]
        actors = [c["name"] for c in data.get("cast", [])[:6]] # å–å‰6ä½
        return {"directors": directors, "actors": actors}
    except:
        return {"directors": [], "actors": []}

def get_reviews(media_type, media_id):
    """è·å–è¯„è®º (ä¼˜å…ˆæ˜¾ç¤ºé•¿è¯„)"""
    url = f"{BASE_URL}/{media_type}/{media_id}/reviews"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=5)
        data = resp.json()
        results = data.get("results", [])
        # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯„è®ºï¼ŒæŒ‰é•¿åº¦é™åº
        valid_reviews = [r for r in results if len(r["content"]) > 50]
        sorted_reviews = sorted(valid_reviews, key=lambda x: len(x["content"]), reverse=True)[:2]
        
        reviews_text = []
        for r in sorted_reviews:
            # æˆªæ–­è¿‡é•¿è¯„è®º
            clean_content = r["content"].strip()[:400]
            reviews_text.append(f"ğŸ‘¤ {r['author']}: {clean_content}...")
        return reviews_text
    except:
        return []

def fetch_content(media_type, existing_ids):
    # 1. è·å– Trending åˆ—è¡¨
    url = f"{BASE_URL}/trending/{media_type}/day"
    params = {"language": "zh-CN"}
    
    try:
        resp = requests.get(url, headers=HEADERS, params=params, timeout=15)
        if resp.status_code != 200: return None
        
        results = resp.json().get("results", [])
        target_item = None
        
        # å»é‡æŸ¥æ‰¾
        for item in results:
            if item["id"] not in existing_ids:
                target_item = item
                break
        
        if not target_item: return None

        # 2. è·å–åŸºç¡€è¯¦æƒ…
        detail_url = f"{BASE_URL}/{media_type}/{target_item['id']}"
        detail_resp = requests.get(detail_url, headers=HEADERS, params=params, timeout=15)
        detail = detail_resp.json()
        
        title = detail.get("title") or detail.get("name")
        original_title = detail.get("original_title") or detail.get("original_name")
        tmdb_overview = detail.get("overview", "")

        # ---------------- æ™ºèƒ½ç®€ä»‹å¢å¼ºé€»è¾‘ ----------------
        final_description = tmdb_overview
        source_note = ""

        # ç­–ç•¥ A: å¦‚æœ TMDB ä¸­æ–‡ç®€ä»‹å¤ªçŸ­ (<30å­—) æˆ–ä¸ºç©º
        if len(tmdb_overview) < 30:
            print(f"   âš ï¸ TMDBç®€ä»‹ä¸è¶³ï¼Œæ­£åœ¨å¯»æ‰¾è¡¥å……èµ„æ–™: {title}")
            
            # å°è¯• 1: ç»´åŸºç™¾ç§‘ (é¦–é€‰)
            wiki_text = get_wikipedia_summary(title)
            if not wiki_text and title != original_title:
                # å¦‚æœä¸­æ–‡æœä¸åˆ°ï¼Œè¯•ä¸€ä¸‹æœåŸå
                wiki_text = get_wikipedia_summary(original_title)
            
            if wiki_text:
                final_description = wiki_text
            else:
                # å°è¯• 2: ç™¾åº¦ç™¾ç§‘ (å¤‡é€‰)
                baidu_text = get_baidu_baike_summary(title)
                if baidu_text:
                    final_description = baidu_text
                else:
                    # å°è¯• 3: è‹±æ–‡ç®€ä»‹ + æç¤º
                    en_overview = get_english_fallback(media_type, detail["id"])
                    if en_overview:
                        final_description = f"(æš‚æ— ä¸­æ–‡ä»‹ç»ï¼ŒåŸæ–‡å¦‚ä¸‹)\n{en_overview}"
        
        # ------------------------------------------------

        # 3. è·å–å…¶ä»–å…ƒæ•°æ®
        ext_ids = get_external_ids(media_type, detail["id"])
        imdb_id = ext_ids.get("imdb_id")
        credits = get_credits(media_type, detail["id"])
        reviews = get_reviews(media_type, detail["id"])
        
        poster = download_image(f"{IMAGE_BASE_URL}{detail.get('poster_path')}", f"{media_type}_{detail['id']}_p.jpg")
        backdrop = download_image(f"{BACKDROP_BASE_URL}{detail.get('backdrop_path')}", f"{media_type}_{detail['id']}_b.jpg")

        return {
            "update_date": datetime.now().strftime("%Y-%m-%d"),
            "id": detail["id"],
            "imdb_id": imdb_id,
            # ç”Ÿæˆè±†ç“£æœç´¢é“¾æ¥ï¼Œè€Œä¸æ˜¯çˆ¬è™«ï¼Œè§„é¿é£é™©
            "douban_link": f"https://search.douban.com/movie/subject_search?search_text={imdb_id}" if imdb_id else f"https://search.douban.com/movie/subject_search?search_text={title}",
            "type": "ç”µå½±" if media_type == "movie" else "å‰§é›†",
            "title": title,
            "original_title": original_title,
            "rating": round(detail.get("vote_average", 0), 1),
            "date": detail.get("release_date") or detail.get("first_air_date"),
            "genres": [g["name"] for g in detail.get("genres", [])],
            "director": credits["directors"],
            "actors": credits["actors"],
            "description": final_description, # è¿™é‡Œæ˜¯å¢å¼ºåçš„ç®€ä»‹
            "reviews": reviews,
            "poster_path": poster,
            "backdrop_path": backdrop
        }

    except Exception as e:
        print(f"âŒ Error fetching {media_type}: {e}")
        return None

# ---------------- ä¸»ç¨‹åº ----------------

def main():
    print("ğŸš€ ä»»åŠ¡å¼€å§‹...")
    is_monday = datetime.today().weekday() == 0
    setup_directories(reset=is_monday)
    
    existing_ids = load_existing_ids()
    new_items = []

    # è·å–ç”µå½±
    print("ğŸ¬ è·å–ç”µå½±...")
    movie = fetch_content("movie", existing_ids)
    if movie: 
        new_items.append(movie)
        existing_ids.append(movie["id"])
        print(f"   âœ… æˆåŠŸè·å–: ã€Š{movie['title']}ã€‹")

    # è·å–å‰§é›†
    print("ğŸ“º è·å–å‰§é›†...")
    tv = fetch_content("tv", existing_ids)
    if tv: 
        new_items.append(tv)
        print(f"   âœ… æˆåŠŸè·å–: ã€Š{tv['title']}ã€‹")

    # ä¿å­˜ç»“æœ
    if new_items:
        current_data = []
        if os.path.exists(JSON_FILE):
            with open(JSON_FILE, 'r', encoding='utf-8') as f:
                current_data = json.load(f)
        
        current_data.extend(new_items)
        
        with open(JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ‰ æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(new_items)} æ¡å†…å®¹ã€‚")
    else:
        print("âš ï¸ æ— æ–°å†…å®¹æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
